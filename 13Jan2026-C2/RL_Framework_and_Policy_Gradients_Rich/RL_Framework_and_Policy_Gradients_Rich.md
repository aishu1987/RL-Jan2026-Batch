# RL Framework and Policy Gradients

> This version embeds **formula images** for easy reading in any Markdown viewer (VS Code, GitHub, etc.).

---

## Learning Outcomes
By the end of this module, learners will be able to:
1. Define an **MDP** and explain the RL interaction loop.
2. Explain what a **policy** is and why we learn it directly in policy-gradient methods.
3. Explain **REINFORCE**: *increase probability of actions that lead to high return*.
4. Read and interpret key RL formulas confidently.
5. Run a minimal **Python REINFORCE** example and explain each component.

---

## 1) RL Framework: the interaction loop

### The RL Loop (memorize)
**Observe state → Choose action → Receive reward + next state → Update policy → Repeat**

At each time step \(t\):
- **State:** \(s_t\)
- **Action:** \(a_t\)
- **Reward:** \(r_{t+1}\)
- **Next state:** \(s_{t+1}\)

---

## 2) Markov Decision Process (MDP)

### 2.1 Definition (MDP tuple)

![MDP tuple](formulas/mdp_tuple.png)

**How to read (say it out loud):**  
“An MDP is defined by a set of **states**, a set of **actions**, a **transition** model, a **reward** function, and a **discount factor**.”

**Meaning of each symbol**
- \(\mathcal{S}\): set of possible states (situations)
- \(\mathcal{A}\): set of possible actions (choices)
- \(P\): transition model (how the environment moves)
- \(R\): reward function (feedback signal)
- \(\gamma\in[0,1)\): discount factor (how much we care about future rewards)

---

### 2.2 Transition model

![Transition model](formulas/transition.png)

**How to read:**  
“Probability of next state \(s'\), **given** current state \(s\) and action \(a\).”  
> Tip: the symbol “\(\mid\)” means **given**.

---

### 2.3 Reward function (common forms)
We often write reward as \(R(s,a)\) or \(R(s,a,s')\).

**Practical meaning**
- Reward can depend on: current state + action  
- or: state + action + next state (outcome matters)

---

### 2.4 Return (long-term reward)

![Return](formulas/return.png)

**How to read:**  
“Return from time \(t\) equals the sum of future rewards, discounted by \(\gamma^k\).”

**Intuition**
- \(\gamma=0\): only immediate reward matters  
- \(\gamma\approx0.99\): long-term reward matters a lot

---

## 3) Policy: what we learn in policy-gradient methods

### 3.1 Stochastic policy

![Stochastic policy](formulas/stochastic_policy.png)

**How to read:**  
“Probability of taking action \(a\) in state \(s\), under parameters \(\theta\).”

**Why stochastic helps**
- Non-zero probability for multiple actions → natural exploration

---

### 3.2 Objective: maximize expected return

![Objective](formulas/objective.png)

**How to read:**  
“\(J(\theta)\) is the expected discounted reward over trajectories generated by the policy.”

---

## 4) Policy Gradients = reward-weighted gradient ascent

### 4.1 Core update (the one to remember)

![Core update](formulas/core_update.png)

**How to read:**  
“Update \(\theta\) by moving in the direction that increases the log-probability of the chosen action, scaled by reward.”

**Plain English intuition**
- High reward → make the chosen action **more likely**
- Low/negative reward → make it **less likely**

---

### 4.2 Why log-probability appears

![Trajectory probability](formulas/traj_prob.png)

Log turns products into sums:

![Log trajectory probability](formulas/log_traj_prob.png)

---

## 5) REINFORCE: policy gradient estimator

### 5.1 REINFORCE gradient

![REINFORCE](formulas/reinforce.png)

**How to read:**  
“Sum over time: return \(G_t\) times the gradient of log-prob of the action taken.”

---

### 5.2 Variance reduction with a baseline (recommended)

![Baseline](formulas/baseline.png)

**How to read:**  
“Use return minus a baseline (expected return) to reduce noise.”

A common baseline is a value function (advantage):

![Advantage](formulas/advantage.png)

---

## 6) Exploration vs Exploitation (Policy Gradient view)

### 6.1 Why exploration is “built in”
Because \(\pi_\theta(a\mid s)\) is stochastic (softmax/Gaussian), the agent explores by sampling.

### 6.2 Control exploration

#### (a) Softmax temperature

![Softmax temperature](formulas/softmax_temp.png)

- \(\tau\) high → more uniform → more exploration  
- \(\tau\) low → more peaked → more exploitation  

#### (b) Entropy bonus (common in PPO/A2C)

![Entropy bonus](formulas/entropy_bonus.png)

- Higher entropy → more randomness  
- \(\beta\) controls exploration strength

---

## 7) Python example: REINFORCE on a tiny MDP (fully self-contained)

### What this demonstrates
- A 2-state MDP where:
  - In **state 0**, action 0 is better
  - In **state 1**, action 1 is better
- REINFORCE learns the policy directly from rewards.

```python
import numpy as np

class TinyMDP:
    def __init__(self, seed=0):
        self.rng = np.random.default_rng(seed)
        self.state = 0

    def reset(self):
        self.state = 0
        return self.state

    def step(self, action):
        s = self.state

        if s == 0:
            if action == 0:
                r = 1.0
                self.state = 0 if self.rng.random() < 0.8 else 1
            else:
                r = 0.0
                self.state = 1 if self.rng.random() < 0.8 else 0
        else:
            if action == 1:
                r = 1.0
                self.state = 1 if self.rng.random() < 0.8 else 0
            else:
                r = 0.0
                self.state = 0 if self.rng.random() < 0.8 else 1

        return self.state, r


def softmax(x):
    x = x - np.max(x)
    e = np.exp(x)
    return e / np.sum(e)

class TabularSoftmaxPolicy:
    def __init__(self, n_states=2, n_actions=2, seed=0):
        self.rng = np.random.default_rng(seed)
        self.theta = 0.01 * self.rng.standard_normal((n_states, n_actions))

    def action_probs(self, s):
        return softmax(self.theta[s])

    def sample_action(self, s):
        p = self.action_probs(s)
        return self.rng.choice(len(p), p=p)

    def grad_log_pi(self, s, a):
        p = self.action_probs(s)
        grad = np.zeros_like(self.theta)
        grad[s, :] = -p
        grad[s, a] += 1.0
        return grad


def train_reinforce(episodes=2000, T=10, gamma=0.95, lr=0.05, seed=0, use_baseline=True):
    env = TinyMDP(seed=seed)
    pi = TabularSoftmaxPolicy(seed=seed)

    baseline = 0.0
    beta = 0.9

    for ep in range(1, episodes + 1):
        s = env.reset()
        states, actions, rewards = [], [], []

        for _ in range(T):
            a = pi.sample_action(s)
            s2, r = env.step(a)
            states.append(s); actions.append(a); rewards.append(r)
            s = s2

        G = np.zeros(T)
        running = 0.0
        for t in reversed(range(T)):
            running = rewards[t] + gamma * running
            G[t] = running

        ep_return = G[0]
        if use_baseline:
            baseline = beta * baseline + (1 - beta) * ep_return

        grad_sum = np.zeros_like(pi.theta)
        for t in range(T):
            advantage = G[t] - baseline if use_baseline else G[t]
            grad_sum += advantage * pi.grad_log_pi(states[t], actions[t])

        pi.theta += lr * grad_sum

        if ep % 200 == 0:
            print("ep", ep, "pi(s0)", np.round(pi.action_probs(0), 3), "pi(s1)", np.round(pi.action_probs(1), 3))

    return pi

if __name__ == "__main__":
    learned = train_reinforce()
    print("Final:", learned.action_probs(0), learned.action_probs(1))
```

---

## 8) Mapping: Math ↔ Code (quick learner view)
- \(\pi_\theta(a\mid s)\) → `action_probs(s)`
- sample \(a\sim\pi_\theta\) → `sample_action(s)`
- \(\nabla_\theta \log \pi_\theta(a\mid s)\) → `grad_log_pi(s, a)`
- \(G_t\) → computed in reversed loop
- baseline \(b\) → `baseline` running average
- update \(\theta \leftarrow \theta + \alpha \sum_t (G_t-b)\nabla \log \pi\) → `pi.theta += lr * grad_sum`

---

## 9) Key Takeaways (exam style)
1. MDP defines **dynamics**: \(P(s'\mid s,a)\) and reward \(R\).
2. Policy gradients optimize **policy directly** (not Q-table).
3. REINFORCE increases probability of actions proportional to **return**.
4. Baselines reduce **variance** and stabilize learning.
5. Exploration is managed via stochastic policies + temperature/entropy bonuses.
